# src/compass_brca/step03_clean_and_normalize.py
# DEFINITIVE VERSION V3 (The Resilient Auto-Medic)
# This version is an expert at handling both .csv and .tsv files.
import sys
import polars as pl
from pathlib import Path
import re

try:
    project_root = Path(__file__).resolve().parents[2]
    sys.path.append(str(project_root / "src"))
    from compass_brca.pipeline_config import RAW_DATA_DIR, INTERIM_DATA_DIR
except ImportError as e:
    sys.exit(f"Fatal Error: Could not import config. Details: {e}")

from rich.console import Console
from rich.progress import track

console = Console()
# --- UPDATED: .tsv is now a supported format ---
SUPPORTED_EXTENSIONS = {'.csv', '.tsv'}

def sanitize_filename(filename: str) -> str:
    sanitized = re.sub(r'[\\/*?:"<>|]', "_", filename)
    return re.sub(r'[,;]', "_", sanitized)

def process_file_with_polars(file_path: Path, output_dir: Path):
    """
    Reads a supported file (.csv or .tsv), cleans it, and saves as Parquet.
    This version is resilient to common encoding and structural errors.
    """
    df = None
    try:
        common_csv_kwargs = {
            "infer_schema_length": 10000,
            "ignore_errors": True,
            "truncate_ragged_lines": True, 
        }

        try:
            if file_path.suffix.lower() == '.csv':
                df = pl.read_csv(file_path, **common_csv_kwargs, encoding='utf-8')
            # --- THIS IS THE CRITICAL LOGIC FOR TSV ---
            elif file_path.suffix.lower() == '.tsv':
                df = pl.read_csv(file_path, separator='\t', **common_csv_kwargs, encoding='utf-8')
        except pl.exceptions.ComputeError as e:
            if "invalid utf-8 sequence" in str(e):
                console.log(f"[yellow]UTF-8 failed for {file_path.name}. Retrying with 'latin-1' encoding.[/]")
                if file_path.suffix.lower() == '.csv':
                    df = pl.read_csv(file_path, **common_csv_kwargs, encoding='latin-1')
                elif file_path.suffix.lower() == '.tsv':
                    df = pl.read_csv(file_path, separator='\t', **common_csv_kwargs, encoding='latin-1')
            else:
                raise e

        if df is not None and len(df) > 0:
            new_filename = sanitize_filename(file_path.stem) + ".parquet"
            output_path = output_dir / file_path.parent.name / new_filename
            output_path.parent.mkdir(parents=True, exist_ok=True)
            df.write_parquet(output_path)
        else:
             console.log(f"[dim]Skipping empty or unreadable file: {file_path.name}[/]")

    except Exception as e:
        console.print(f"[bold red]Critical error processing {file_path.name}: {e}[/]")

def main():
    console.rule("[bold magenta]Step 03: Clean and Normalize Data (Resilient V3)[/]")
    if not RAW_DATA_DIR.exists():
        console.print(f"[bold red]Error: Raw data directory not found at {RAW_DATA_DIR}[/]"); sys.exit(1)
        
    INTERIM_DATA_DIR.mkdir(parents=True, exist_ok=True)
    files_to_process = [p for p in RAW_DATA_DIR.rglob("*") if p.is_file() and p.suffix.lower() in SUPPORTED_EXTENSIONS]
    
    if not files_to_process:
        console.print("[yellow]No supported text files (.csv, .tsv) found to process.[/]"); return
        
    console.print(f"Found {len(files_to_process)} supported files to convert to Parquet using Polars.")
    
    for file_path in track(files_to_process, description="Normalizing files..."):
        process_file_with_polars(file_path, INTERIM_DATA_DIR)
        
    console.rule("[bold green]Step 03: Clean and Normalize Data - COMPLETED[/]")

if __name__ == "__main__":
    main()